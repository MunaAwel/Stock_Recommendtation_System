{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1809b78-b9cf-4fef-9a80-742769ac4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment cache...\n",
      "Sentiment cache loaded.\n",
      "Loading stock data...\n",
      "Data loaded: 1048575 records found.\n",
      "Preparing features...\n",
      "Standardizing features and applying K-Means clustering...\n",
      "Labeling risk levels...\n",
      "Select your risk preference:\n",
      "a. Low\n",
      "b. Medium\n",
      "c. High\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (a/b/c):  a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommending top 5 unique Low-risk stocks...\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+------+------------+---------+--------------+-----+----------+-----------+-------------------+------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+----------+----------+-------------------+\n",
      "|Date      |Open       |High       |Low        |Close      |Adj Close  |Volume |Ticker|Returns     |PE_Ratio |Dividend_Yield|Beta |Avg_Volume|Volatility |Sentiment_Score    |features                                                          |scaled_features                                                                                                    |scaled_values                                                                                                           |prediction|Risk_Level|Composite_Score    |\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+------+------------+---------+--------------+-----+----------+-----------+-------------------+------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+----------+----------+-------------------+\n",
      "|8/3/2021  |42.09000015|42.36000061|41.91999817|42.33000183|37.75608063|99900  |CHT   |0.008577597 |24.668833|0.038599998   |0.064|94659     |0.117398386|0.1333             |[0.117398386,94659.0,24.668833,0.038599998,0.064,0.1333]          |[1.1687439509391084,0.006644321140093017,0.5425733543566423,1.629615049455998,0.14070705101382475,0.96095582373625]|[1.1687439509391084, 0.006644321140093017, 0.5425733543566423, 1.629615049455998, 0.14070705101382475, 0.96095582373625]|0         |Low       |0.07960559109999998|\n",
      "|4/25/2023 |78.04000092|78.55999756|77.51000214|77.69999695|75.42559814|6437500|CL    |-0.002311313|26.030928|0.0199        |0.397|4163131   |0.208259962|0.34367499999999995|[0.208259962,4163131.0,26.030928,0.0199,0.397,0.34367499999999995]|[2.073304234440741,0.2922192217567963,0.5725316605765762,0.8401383721360389,0.8728234258201317,2.4775430811894648] |[2.073304234440741, 0.2922192217567963, 0.5725316605765762, 0.8401383721360389, 0.8728234258201317, 2.4775430811894648] |0         |Low       |0.1464753673       |\n",
      "|10/20/2023|149.0500031|149.8800049|147.9199982|148.0500031|145.3030853|7283900|PG    |-0.001349072|22.751345|0.0239        |0.407|6944595   |0.211612588|0.19493749999999999|[0.211612588,6944595.0,22.751345,0.0239,0.407,0.19493749999999999]|[2.106680854774015,0.48745623097522967,0.5003995759659658,1.0090104067362478,0.8948089025410417,1.4052987688641052]|[2.106680854774015, 0.48745623097522967, 0.5003995759659658, 1.0090104067362478, 0.8948089025410417, 1.4052987688641052]|0         |Low       |0.14853353319999998|\n",
      "|9/30/2020 |146.9400024|148.3200073|146.6799927|147.6600037|128.145874 |1578800|KMB   |0.009020448 |18.537157|0.0344        |0.402|2075801   |0.216768695|0.23451249999999998|[0.216768695,2075801.0,18.537157,0.0344,0.402,0.23451249999999998]|[2.158011789312117,0.14570498808276258,0.40771152221613866,1.4522994975617958,0.8838161641805867,1.690593793052868]|[2.158011789312117, 0.14570498808276258, 0.40771152221613866, 1.4522994975617958, 0.8838161641805867, 1.690593793052868]|0         |Low       |0.1490319521       |\n",
      "|4/6/2022  |63.04999924|63.59000015|62.72000122|63.20999908|59.54660797|5389600|MDLZ  |0.003174019 |19.84637 |0.026500002   |0.549|5988426   |0.218230362|0.2296375          |[0.218230362,5988426.0,19.84637,0.026500002,0.549,0.2296375]      |[2.1725632198959866,0.4203406487252418,0.4365067266336854,1.1187773136624009,1.2070026719779656,1.655450059814202] |[2.1725632198959866, 0.4203406487252418, 0.4365067266336854, 1.1187773136624009, 1.2070026719779656, 1.655450059814202] |0         |Low       |0.15180904769999998|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+------+------------+---------+--------------+-----+----------+-----------+-------------------+------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+----------+----------+-------------------+\n",
      "\n",
      "Plotting returns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sentiment cache...\n",
      "Sentiment cache saved.\n",
      "Saving output to HTML...\n",
      "HTML output saved to /shared_data/output.html\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, isnan\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "import yfinance as yf\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StockRiskAssessment\").getOrCreate()\n",
    "\n",
    "# Global sentiment cache\n",
    "sentiment_cache = {}\n",
    "\n",
    "# Step 1: Load data from CSV file\n",
    "def load_stock_data(filepath):\n",
    "    print(\"Loading stock data...\")\n",
    "    stock_df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
    "    print(f\"Data loaded: {stock_df.count()} records found.\")\n",
    "    return stock_df\n",
    "\n",
    "# Step 2: Prepare features for clustering\n",
    "def prepare_features(stock_df, broadcast_sentiment_cache):\n",
    "    print(\"Preparing features...\")\n",
    "    stock_df = stock_df.dropna(subset=['Volatility', 'Avg_Volume', 'PE_Ratio', 'Dividend_Yield', 'Beta', 'Returns'])\n",
    "\n",
    "    # Sentiment analysis using UDF\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    @udf(DoubleType())\n",
    "    def analyze_sentiment(ticker):\n",
    "        sentiment_cache = broadcast_sentiment_cache.value  # Access the broadcasted sentiment_cache\n",
    "        if ticker in sentiment_cache:\n",
    "            return sentiment_cache[ticker]  # Use cached value\n",
    "        news = yf.Ticker(ticker).news\n",
    "        if not news:\n",
    "            sentiment_cache[ticker] = 0.0\n",
    "            return 0.0\n",
    "        scores = [analyzer.polarity_scores(item['title'])['compound'] for item in news]\n",
    "        sentiment_cache[ticker] = float(sum(scores) / len(scores)) if scores else 0.0\n",
    "        return sentiment_cache[ticker]\n",
    "\n",
    "    stock_df = stock_df.withColumn(\"Sentiment_Score\", analyze_sentiment(col(\"Ticker\")))\n",
    "\n",
    "    # Select features\n",
    "    feature_columns = ['Volatility', 'Avg_Volume', 'PE_Ratio', 'Dividend_Yield', 'Beta', 'Sentiment_Score']\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    stock_df = assembler.transform(stock_df)\n",
    "    return stock_df\n",
    "\n",
    "# Step 3: Apply K-Means clustering\n",
    "def apply_kmeans(stock_df, n_clusters=3):\n",
    "    print(\"Standardizing features and applying K-Means clustering...\")\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scaled_df = scaler.fit(stock_df).transform(stock_df)\n",
    "    \n",
    "    # Extracting the 'values' field from 'scaled_features' (vector) to an array using vector_to_array\n",
    "    scaled_df = scaled_df.withColumn(\"scaled_values\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "    # Check for any missing data in 'scaled_values'\n",
    "    missing_data = scaled_df.filter(\n",
    "        col(\"scaled_values\").isNull()\n",
    "    ).count()\n",
    "\n",
    "    if missing_data > 0:\n",
    "        print(f\"Warning: {missing_data} rows with missing values in scaled_features detected. Dropping them.\")\n",
    "        scaled_df = scaled_df.filter(col(\"scaled_values\").isNotNull())\n",
    "\n",
    "    # Repartition the dataframe to ensure we have evenly distributed partitions for KMeans\n",
    "    scaled_df = scaled_df.repartition(10)  # You can adjust the number of partitions depending on your system\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(featuresCol=\"scaled_features\", k=n_clusters, seed=42)\n",
    "    model = kmeans.fit(scaled_df)\n",
    "    clustered_df = model.transform(scaled_df)\n",
    "    \n",
    "    return clustered_df\n",
    "\n",
    "# Step 4: Add risk levels to the dataset\n",
    "def label_risk_levels(clustered_df):\n",
    "    print(\"Labeling risk levels...\")\n",
    "    risk_mapping = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "    risk_udf = udf(lambda x: risk_mapping[x], StringType())\n",
    "    clustered_df = clustered_df.withColumn(\"Risk_Level\", risk_udf(col(\"prediction\")))\n",
    "    return clustered_df\n",
    "\n",
    "# Step 5: Recommend top 5 unique stocks\n",
    "def recommend_top_stocks(clustered_df, risk_level='Low', top_n=5):\n",
    "    print(f\"Recommending top {top_n} unique {risk_level}-risk stocks...\")\n",
    "    filtered_df = clustered_df.filter(col(\"Risk_Level\") == risk_level)\n",
    "    unique_stocks = filtered_df.dropDuplicates([\"Ticker\"])\n",
    "\n",
    "    if unique_stocks.count() < top_n:\n",
    "        print(f\"Not enough stocks for {risk_level} risk level. Providing all available stocks.\")\n",
    "        return unique_stocks\n",
    "\n",
    "    unique_stocks = unique_stocks.withColumn(\"Composite_Score\", \n",
    "        col(\"Volatility\") * 0.7 + col(\"Returns\") * -0.3)\n",
    "    recommended_stocks = unique_stocks.orderBy(col(\"Composite_Score\")).limit(top_n)\n",
    "\n",
    "    # Rearranging columns for better readability\n",
    "    recommended_stocks.show(truncate=False)\n",
    "    return recommended_stocks\n",
    "\n",
    "# Step 6: Save sentiment cache to HDFS\n",
    "def save_sentiment_cache_to_hdfs(sentiment_cache, hdfs_path):\n",
    "    print(\"Saving sentiment cache...\")\n",
    "    try:\n",
    "        # Save the sentiment_cache dictionary to a JSON file\n",
    "        with open(hdfs_path, 'w') as f:\n",
    "            json.dump(sentiment_cache, f)\n",
    "        print(\"Sentiment cache saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save sentiment cache. ({e})\")\n",
    "\n",
    "# Step 7: Load sentiment cache from HDFS\n",
    "def load_sentiment_cache_from_hdfs(hdfs_path):\n",
    "    print(\"Loading sentiment cache...\")\n",
    "    try:\n",
    "        # Read the JSON file as a dictionary\n",
    "        with open(hdfs_path, 'r') as f:\n",
    "            sentiment_dict = json.load(f)\n",
    "\n",
    "        # Ensure all sentiment scores are floats to avoid type mismatch\n",
    "        sentiment_dict = {ticker: float(score) for ticker, score in sentiment_dict.items()}\n",
    "\n",
    "        # Update the global sentiment_cache dictionary\n",
    "        global sentiment_cache\n",
    "        sentiment_cache = sentiment_dict\n",
    "        \n",
    "        print(\"Sentiment cache loaded.\")\n",
    "        return sentiment_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load sentiment cache. Starting fresh. ({e})\")\n",
    "        return {}\n",
    "\n",
    "# Step 8: Plot cumulative returns for recommended stocks\n",
    "def plot_returns(recommended_stocks, start_date, end_date):\n",
    "    print(\"Plotting returns...\")\n",
    "    # Extract ticker symbols into a list\n",
    "    tickers = [row['Ticker'] for row in recommended_stocks.select('Ticker').collect()]\n",
    "    \n",
    "    sp500_data = yf.download('^GSPC', start=start_date, end=end_date)\n",
    "    sp500_data['Returns'] = sp500_data['Adj Close'].pct_change()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=sp500_data.index, y=sp500_data['Returns'].cumsum(), mode='lines', name='S&P 500'))\n",
    "\n",
    "    for ticker in tickers:\n",
    "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        stock_data['Returns'] = stock_data['Adj Close'].pct_change()\n",
    "        fig.add_trace(go.Scatter(x=stock_data.index, y=stock_data['Returns'].cumsum(), mode='lines', name=ticker))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Cumulative Returns of Top 5 Recommended Stocks vs. S&P 500\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Cumulative Returns\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Step 9: Save output to HTML\n",
    "def save_output_to_html(recommended_stocks, plot, html_file):\n",
    "    print(\"Saving output to HTML...\")\n",
    "    recommended_stocks_pd = recommended_stocks.toPandas()\n",
    "    table_html = recommended_stocks_pd.to_html(index=False, border=0, classes=\"table table-bordered table-striped\")\n",
    "\n",
    "    html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <style>\n",
    "            table {{ width: 100%; border-collapse: collapse; }}\n",
    "            th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h2>Top 5 Recommended Stocks for {recommended_stocks_pd['Risk_Level'][0]} Risk Level</h2>\n",
    "        {table_html}\n",
    "        {plot.to_html(full_html=False)}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    with open(html_file, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"HTML output saved to {html_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"/shared_data/all_stock_data_with_metrics.csv\"\n",
    "    hdfs_path = \"/shared_data/sentiment_cache.json\"\n",
    "    html_output_path = \"/shared_data/output.html\"  # Updated for clarity\n",
    "\n",
    "    sentiment_cache = load_sentiment_cache_from_hdfs(hdfs_path)\n",
    "    \n",
    "    # Broadcast sentiment cache to all worker nodes\n",
    "    broadcast_sentiment_cache = spark.sparkContext.broadcast(sentiment_cache)\n",
    "\n",
    "    stock_df = load_stock_data(filepath)\n",
    "    stock_df = prepare_features(stock_df, broadcast_sentiment_cache)\n",
    "    clustered_df = apply_kmeans(stock_df)\n",
    "    clustered_df = label_risk_levels(clustered_df)\n",
    "\n",
    "    print(\"Select your risk preference:\")\n",
    "    print(\"a. Low\")\n",
    "    print(\"b. Medium\")\n",
    "    print(\"c. High\")\n",
    "    choice = input(\"Enter your choice (a/b/c): \").lower()\n",
    "    risk_mapping = {'a': 'Low', 'b': 'Medium', 'c': 'High'}\n",
    "    risk_level = risk_mapping.get(choice, 'Low')\n",
    "\n",
    "    recommended_stocks = recommend_top_stocks(clustered_df, risk_level, top_n=5)\n",
    "    start_date = '2019-01-01'\n",
    "    end_date = '2024-01-01'\n",
    "    plot = plot_returns(recommended_stocks, start_date, end_date)\n",
    "\n",
    "    save_sentiment_cache_to_hdfs(sentiment_cache, hdfs_path)\n",
    "    save_output_to_html(recommended_stocks, plot, html_output_path)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f44211-4bff-480b-bb37-5277b265c0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
